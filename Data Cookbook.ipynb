* By Sarena Yousuf
* CS215 Final Project
* Spring 2025
* Professor Wirfs-Brock

# Intro: About the Project 
# This Data Science Cookbook is a curated collection of essential techniques and code snippets I learned and applied throughout the semester. 
# Each recipe is designed to showcase practical data science skills, complete with real-world examples using datasets from various projects I worked on during the course.

## Note:
## For the last two methods, I included code that explains the recipes better before showing my code. 
## Since the requirement was that a student from CS167 should be understand, I felt more explaination was necessary since these methods are a bit complex. 


# Recipe 1: Extracting and Flattening Nested API JSON Data into a DataFrame

# data used: assessed through the API

# Getting the data
NASA_API_KEY = "private API key" 
# I used a API key that I cannot share publically, 
# but to run this code, go to https://api.nasa.gov/ to find get the key

today = datetime.date.today()
start_date = today - datetime.timedelta(days=6) # using this format so it is relevant to the day the code is run
end_date = today

start_date_str = start_date.isoformat()
end_date_str = end_date.isoformat()

feed_url = "https://api.nasa.gov/neo/rest/v1/feed"
params = {"start_date": start_date_str, "end_date": end_date_str, "api_key": NASA_API_KEY}

response = requests.get(feed_url, params=params)
data = response.json()
print("Fetched data from", start_date_str, "to", end_date_str)


## my application

# Import necessary libraries
import requests  # For making API requests
import pandas as pd  # For handling tabular data

# Define the API endpoint and parameters
url = "https://api.nasa.gov/neo/rest/v1/feed"
params = {
    "start_date": "2023-01-01",  # Start date for asteroid data
    "end_date": "2023-01-07",    # End date for asteroid data
    "api_key": "DEMO_KEY"        # Use your NASA API key here
}

# Send a GET request to the API
response = requests.get(url, params=params)

# Convert the JSON response into a Python dictionary
data = response.json()

# Initialize an empty list to store asteroid data
asteroids = []

# Loop through the nested JSON structure to extract relevant details
for date, objects in data['near_earth_objects'].items():
    for obj in objects:
        # Extract the first close approach data for each asteroid
        close_approach = obj['close_approach_data'][0]

        # Append the extracted information to the asteroids list
        asteroids.append({
            'Date': date,
            'Name': obj['name'],
            'Diameter_Max_Meters': obj['estimated_diameter']['meters']['estimated_diameter_max'],
            'Hazardous': obj['is_potentially_hazardous_asteroid'],
            'Relative_Velocity_km_s': float(close_approach['relative_velocity']['kilometers_per_second']),
            'Miss_Distance_km': float(close_approach['miss_distance']['kilometers']),
            'Orbiting_Body': close_approach['orbiting_body']
        })

# Convert the extracted data into a Pandas DataFrame for analysis
df_asteroids = pd.DataFrame(asteroids)
print(df_asteroids.head())  # Print the first few rows of the DataFrame

# Recipe 2: Creating New Time Features and Grouping by Weekday

# Import necessary libraries
import pandas as pd

# Load the timestamped data from Project 7 file
df_time = pd.read_csv('yt_timestampsf_final.csv')  

# Convert the timestamp column to datetime format for easier manipulation
df_time['timestamp'] = pd.to_datetime(df_time['timestamp'])

# Extract new features from the timestamp
df_time['year'] = df_time['timestamp'].dt.year       # Year of the timestamp
df_time['month'] = df_time['timestamp'].dt.month     # Month of the timestamp
df_time['weekday'] = df_time['timestamp'].dt.day_name()  # Day of the week
df_time['hour'] = df_time['timestamp'].dt.hour       # Hour of the day

# Group data by weekday and compute the mean of the 'value' column
# Reindex to show weekdays in order
weekday_summary = df_time.groupby('weekday')['value'].mean().reindex([
    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
])

# Display the summary
print(weekday_summary)

# Recipe 3: Two Ways to Extract Nested Data from an API Response

# Import necessary libraries
import requests
import pandas as pd

# Fetch data from NASA's Near Earth Object Web Service (NEOWS) API
url = "https://api.nasa.gov/neo/rest/v1/feed"
params = {
    "start_date": "2023-01-01",  # Start date for asteroid data
    "end_date": "2023-01-07",    # End date for asteroid data
    "api_key": "DEMO_KEY"        # Replace with your NASA API key
}

# Make the API request
response = requests.get(url, params=params)
data = response.json()

# Two ways to extract nested data

# Method 1: Using Loops to Extract and Flatten Nested Data
asteroids_list_loop = []
for date, objects in data['near_earth_objects'].items():
    for obj in objects:
        close_approach = obj['close_approach_data'][0]
        asteroids_list_loop.append({
            'Name': obj['name'],
            'Diameter_Max_Meters': obj['estimated_diameter']['meters']['estimated_diameter_max'],
            'Hazardous': obj['is_potentially_hazardous_asteroid'],
            'Velocity_km_s': float(close_approach['relative_velocity']['kilometers_per_second']),
            'Miss_Distance_km': float(close_approach['miss_distance']['kilometers']),
            'Orbiting_Body': close_approach['orbiting_body']
        })

# Convert the loop-based data extraction to a DataFrame
df_asteroids_loop = pd.DataFrame(asteroids_list_loop)

# Method 2: Using List Comprehension to Extract and Flatten Nested Data
asteroids_list_comprehension = [
    {
        'Name': obj['name'],
        'Diameter_Max_Meters': obj['estimated_diameter']['meters']['estimated_diameter_max'],
        'Hazardous': obj['is_potentially_hazardous_asteroid'],
        'Velocity_km_s': float(obj['close_approach_data'][0]['relative_velocity']['kilometers_per_second']),
        'Miss_Distance_km': float(obj['close_approach_data'][0]['miss_distance']['kilometers']),
        'Orbiting_Body': obj['close_approach_data'][0]['orbiting_body']
    }
    for date, objects in data['near_earth_objects'].items()
    for obj in objects
]

# Convert the list comprehension data to a DataFrame
df_asteroids_comprehension = pd.DataFrame(asteroids_list_comprehension)

# Display the first few rows from both methods
print("Data extracted using loop:")
print(df_asteroids_loop.head())

print("\nData extracted using list comprehension:")
print(df_asteroids_comprehension.head())

# Recipe 4: Performing Sentiment Analysis on News Articles

""" Step-by-step explanation:

We initialize VADER's SentimentIntensityAnalyzer, a pre-trained model for sentiment analysis
The analyzer produces four scores for each text:

* neg: Proportion of negative sentiment (0-1)
* neu: Proportion of neutral sentiment (0-1)
* pos: Proportion of positive sentiment (0-1)
* compound: Overall sentiment (-1 to +1, where negative values indicate negative sentiment)


We apply the analyzer to each article in both datasets
We calculate the average sentiment scores for each dataset
Finally, we compare the differences to identify sentiment patterns


"""
# Import necessary libraries
import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk

# Download the necessary VADER lexicon
nltk.download('vader_lexicon', quiet=True)

def perform_sentiment_analysis(dataset1_df, dataset2_df):
    """
    Perform sentiment analysis on two datasets and compare results

    Parameters:
    dataset1_df (DataFrame): First dataset with 'text' column
    dataset2_df (DataFrame): Second dataset with 'text' column

    Returns:
    DataFrame: Comparison of sentiment scores between datasets
    """
    print("Performing sentiment analysis...")

    # Initialize the VADER sentiment analyzer
    analyzer = SentimentIntensityAnalyzer()

    # Define function to get sentiment scores for a text
    def get_sentiment(text):
        if not isinstance(text, str):
            return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}
        return analyzer.polarity_scores(text)

    # Calculate sentiment for both datasets
    dataset1_sentiments = pd.DataFrame(dataset1_df['text'].apply(get_sentiment).tolist())
    dataset2_sentiments = pd.DataFrame(dataset2_df['text'].apply(get_sentiment).tolist())

    # Calculate average sentiment scores
    dataset1_avg = dataset1_sentiments.mean()
    dataset2_avg = dataset2_sentiments.mean()

    # Create comparison DataFrame
    sentiment_comparison = pd.DataFrame({
        'Sentiment': ['Negative', 'Neutral', 'Positive', 'Compound'],
        'Dataset1': [dataset1_avg['neg'], dataset1_avg['neu'], dataset1_avg['pos'], dataset1_avg['compound']],
        'Dataset2': [dataset2_avg['neg'], dataset2_avg['neu'], dataset2_avg['pos'], dataset2_avg['compound']],
        'Difference': [
            dataset1_avg['neg'] - dataset2_avg['neg'],
            dataset1_avg['neu'] - dataset2_avg['neu'],
            dataset1_avg['pos'] - dataset2_avg['pos'],
            dataset1_avg['compound'] - dataset2_avg['compound']
        ]
    })

    return sentiment_comparison



## Example from my code


# Initialize NewsAPI with API key 
api_key = 'private api key' 
# I used a private API key that I cannot share publically 
# but to run this code, please request your own at https://newsapi.org/account after you create your account.
newsapi = NewsApiClient(api_key=api_key)

def fetch_and_process_articles(query='Kashmir', sources='the-times-of-india,the-hindu', year='2024'):
    """Fetch articles and preprocess them in one step"""
    print(f"Fetching and processing articles for query: '{query}'")
    
    # Fetch articles
    all_articles = newsapi.get_everything(
        q=f"{query} {year}",
        sources=sources,
        language='en',
        sort_by='relevancy',
        page_size=100
    )
    
    # Create DataFrame
    df = pd.DataFrame([
        {
            'title': article['title'],
            'description': article['description'],
            'content': article['content'],
            'source': article['source']['name'],
            'publishedAt': article['publishedAt'],
            'url': article['url']
        }
        for article in all_articles['articles']
    ])
    
    # Add text field and clean
    df['text'] = df['title'].fillna('') + ' ' + df['description'].fillna('')
    df['clean_text'] = df['text'].apply(clean_text)
    df['clean_title'] = df['title'].apply(clean_text)
    df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))
    
    print(f"Retrieved {len(df)} articles with average {df['word_count'].mean():.1f} words per article")
    return df



def perform_sentiment_analysis(kashmir_df, baseline_df):
    """Perform sentiment analysis on both datasets (new technique)"""
    print("Performing sentiment analysis...")
    
    # Initialize analyzer
    analyzer = SentimentIntensityAnalyzer()
    
    # Analyze sentiment
    def get_sentiment(text):
        if not isinstance(text, str):
            return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}
        return analyzer.polarity_scores(text)
    
    # Calculate sentiment for both datasets
    kashmir_sentiments = pd.DataFrame(kashmir_df['text'].apply(get_sentiment).tolist())
    baseline_sentiments = pd.DataFrame(baseline_df['text'].apply(get_sentiment).tolist())
    
    # Calculate averages
    kashmir_avg = kashmir_sentiments.mean()
    baseline_avg = baseline_sentiments.mean()
    
    # Create comparison DataFrame
    sentiment_comparison = pd.DataFrame({
        'Sentiment': ['Negative', 'Neutral', 'Positive', 'Compound'],
        'Kashmir': [kashmir_avg['neg'], kashmir_avg['neu'], kashmir_avg['pos'], kashmir_avg['compound']],
        'General News': [baseline_avg['neg'], baseline_avg['neu'], baseline_avg['pos'], baseline_avg['compound']],
        'Difference': [
            kashmir_avg['neg'] - baseline_avg['neg'], 
            kashmir_avg['neu'] - baseline_avg['neu'],
            kashmir_avg['pos'] - baseline_avg['pos'],
            kashmir_avg['compound'] - baseline_avg['compound']
        ]
    })
    
    return sentiment_comparison





# Recipe 5: Finding Distinctive Words Using TF-IDF Analysis
"""
TF-IDF analysis is extremely powerful for identifying what makes one set of documents distinctive from another.

Step-by-step explanation:

We combine texts from both datasets into a single DataFrame with labels
We initialize a TF-IDF vectorizer with parameters to control word frequency thresholds
The vectorizer transforms the text into a matrix where:

Rows represent documents
Columns represent unique words
Values represent the TF-IDF score of each word in each document


We calculate the mean TF-IDF score for each word in each dataset
We compute the difference in mean scores to identify words that are distinctively associated with each dataset
Finally, we sort to find the most overrepresented and underrepresented words
"""
# Import necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

def perform_tfidf_analysis(dataset1_df, dataset2_df, column_name='clean_text'):
    """
    Calculate TF-IDF to find distinctive words between two datasets

    Parameters:
    dataset1_df (DataFrame): First dataset
    dataset2_df (DataFrame): Second dataset
    column_name (str): Column containing text to analyze

    Returns:
    tuple: (DataFrame of overrepresented words, DataFrame of underrepresented words)
    """
    print("Performing TF-IDF analysis...")

    # Combine documents and create labels
    all_docs = list(dataset1_df[column_name]) + list(dataset2_df[column_name])
    doc_classes = [1] * len(dataset1_df) + [0] * len(dataset2_df)
    docs_df = pd.DataFrame({'text': all_docs, 'is_dataset1': doc_classes})

    # Initialize and fit TF-IDF vectorizer
    vectorizer = TfidfVectorizer(
        min_df=1,           # Minimum document frequency
        max_df=0.95,        # Maximum document frequency
        use_idf=True,       # Use inverse document frequency
        smooth_idf=True,    # Add 1 to document frequencies to prevent division by zero
        sublinear_tf=True   # Apply sublinear scaling to term frequencies
    )
    tfidf_matrix = vectorizer.fit_transform(docs_df['text'])
    feature_names = vectorizer.get_feature_names_out()

    # Calculate mean TF-IDF for each class
    dataset1_indices = docs_df[docs_df['is_dataset1'] == 1].index
    dataset2_indices = docs_df[docs_df['is_dataset1'] == 0].index
    dataset1_tfidf = tfidf_matrix[dataset1_indices].mean(axis=0).A1
    dataset2_tfidf = tfidf_matrix[dataset2_indices].mean(axis=0).A1

    # Create DataFrame with differences
    tfidf_df = pd.DataFrame({
        'word': feature_names,
        'dataset1_tfidf': dataset1_tfidf,
        'dataset2_tfidf': dataset2_tfidf,
        'diff': dataset1_tfidf - dataset2_tfidf
    })

    # Sort for over and under-represented words
    overrepresented = tfidf_df.sort_values('diff', ascending=False)
    underrepresented = tfidf_df.sort_values('diff', ascending=True)

    print(f"Top overrepresented term: '{overrepresented.iloc[0]['word']}' (diff: {overrepresented.iloc[0]['diff']:.4f})")
    print(f"Top underrepresented term: '{underrepresented.iloc[0]['word']}' (diff: {underrepresented.iloc[0]['diff']:.4f})")

    return overrepresented, underrepresented


## Example from my code


# Initialize NewsAPI with API key
api_key = 'private API key' # Used a private API key that I cannot share publically but all you need to do to get your own is go to newsapi and request a private API key. 
newsapi = NewsApiClient(api_key=api_key)

def fetch_and_process_articles(query='Kashmir', sources='the-times-of-india,the-hindu', year='2024'):
    """Fetch articles and preprocess them in one step"""
    print(f"Fetching and processing articles for query: '{query}'")
    
    # Fetch articles
    all_articles = newsapi.get_everything(
        q=f"{query} {year}",
        sources=sources,
        language='en',
        sort_by='relevancy',
        page_size=100
    )
    
    # Create DataFrame
    df = pd.DataFrame([
        {
            'title': article['title'],
            'description': article['description'],
            'content': article['content'],
            'source': article['source']['name'],
            'publishedAt': article['publishedAt'],
            'url': article['url']
        }
        for article in all_articles['articles']
    ])
    
    # Add text field and clean
    df['text'] = df['title'].fillna('') + ' ' + df['description'].fillna('')
    df['clean_text'] = df['text'].apply(clean_text)
    df['clean_title'] = df['title'].apply(clean_text)
    df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))
    
    print(f"Retrieved {len(df)} articles with average {df['word_count'].mean():.1f} words per article")
    return df

def perform_tfidf_analysis(kashmir_df, baseline_df):
    """Calculate TF-IDF to find distinctive words between datasets"""
    print("Performing TF-IDF analysis...")
    
    # Combine documents and create labels
    all_docs = list(kashmir_df['clean_text']) + list(baseline_df['clean_text'])
    doc_classes = [1] * len(kashmir_df) + [0] * len(baseline_df)
    docs_df = pd.DataFrame({'text': all_docs, 'is_kashmir': doc_classes})
    
    # Initialize and fit TF-IDF vectorizer
    vectorizer = TfidfVectorizer(min_df=1, max_df=0.95, use_idf=True, smooth_idf=True, sublinear_tf=True)
    tfidf_matrix = vectorizer.fit_transform(docs_df['text'])
    feature_names = vectorizer.get_feature_names_out()
    
    # Calculate mean TF-IDF for each class
    kashmir_indices = docs_df[docs_df['is_kashmir'] == 1].index
    baseline_indices = docs_df[docs_df['is_kashmir'] == 0].index
    kashmir_tfidf = tfidf_matrix[kashmir_indices].mean(axis=0).A1
    baseline_tfidf = tfidf_matrix[baseline_indices].mean(axis=0).A1
    
    # Create DataFrame with differences
    tfidf_df = pd.DataFrame({
        'word': feature_names,
        'kashmir_tfidf': kashmir_tfidf,
        'baseline_tfidf': baseline_tfidf,
        'diff': kashmir_tfidf - baseline_tfidf
    })
    
    # Sort for over and under-represented words
    overrepresented = tfidf_df.sort_values('diff', ascending=False)
    underrepresented = tfidf_df.sort_values('diff', ascending=True)
    
    print(f"Top overrepresented term: '{overrepresented.iloc[0]['word']}' (diff: {overrepresented.iloc[0]['diff']:.4f})")
    print(f"Top underrepresented term: '{underrepresented.iloc[0]['word']}' (diff: {underrepresented.iloc[0]['diff']:.4f})")
    
    return overrepresented, underrepresented

